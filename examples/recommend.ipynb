{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3592a605",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Proper Evaluation with Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2291ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zain/anaconda3/envs/matchmaker-dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original interaction data...\n",
      "Train set: 7,862,310 interactions\n",
      "Test set:  1,965,578 interactions\n",
      "Train set: 7,862,310 interactions\n",
      "Test set:  1,965,578 interactions\n",
      "\n",
      "üîÑ Training model on 80% of data...\n",
      "Reading data... \n",
      "üîÑ Training model on 80% of data...\n",
      "Reading data... ‚úÖ\n",
      "Fitting ALS... \n",
      "üöÄ Preparing data...\n",
      "‚úÖ\n",
      "Fitting ALS... \n",
      "üöÄ Preparing data...\n",
      "üéØ Training male‚Üífemale ALS...\n",
      "üéØ Training male‚Üífemale ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 16.93it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Training female‚Üímale ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:00<00:00, 309.95it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Converting factors to CuPy arrays...\n",
      "‚úÖ Trained M2F ALS with 31134 males √ó 32994 females\n",
      "‚úÖ Trained F2M ALS with 9925 females √ó 38446 males\n",
      "Complete! ‚úÖ\n",
      "User DF updated ‚úÖ\n",
      "User DF updated ‚úÖ\n",
      "User DF updated ‚úÖ\n",
      "Building FAISS recommender (pop)... User DF updated ‚úÖ\n",
      "Building FAISS recommender (pop)... ‚úÖ\n",
      "‚úÖ Training complete on train set\n",
      "‚úÖ\n",
      "‚úÖ Training complete on train set\n"
     ]
    }
   ],
   "source": [
    "# Proper train/test split evaluation\n",
    "from matchmaker import MatchingEngine\n",
    "import cudf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load original data and split by timestamp\n",
    "print(\"Loading original interaction data...\")\n",
    "raw_data = cudf.read_csv(\"data/swipes_clean.csv\")\n",
    "\n",
    "# Sort by timestamp and split 80/20\n",
    "raw_data = raw_data.sort_values('timestamp')\n",
    "split_idx = int(len(raw_data) * 0.8)\n",
    "\n",
    "train_data = raw_data.iloc[:split_idx]\n",
    "test_data = raw_data.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train set: {len(train_data):,} interactions\")\n",
    "print(f\"Test set:  {len(test_data):,} interactions\")\n",
    "\n",
    "# Save splits temporarily\n",
    "train_data.to_csv(\"/tmp/train_split.csv\", index=False)\n",
    "test_data.to_csv(\"/tmp/test_split.csv\", index=False)\n",
    "\n",
    "# 2. Build a NEW engine on ONLY the training data\n",
    "print(\"\\nüîÑ Training model on 80% of data...\")\n",
    "engine_test = MatchingEngine()\n",
    "engine_test.load_interactions(\"/tmp/train_split.csv\",\n",
    "    decider_col='decidermemberid',\n",
    "    other_col='othermemberid', \n",
    "    like_col='like', \n",
    "    timestamp_col='timestamp',\n",
    "    gender_col='decidergender')\n",
    "engine_test.run_engagement()\n",
    "engine_test.run_elo()\n",
    "engine_test.build_recommender()\n",
    "print(\"‚úÖ Training complete on train set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e4c54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mutual_compatibility(engine_test, test_data, gender='M', k=100):\n",
    "    \"\"\"\n",
    "    Evaluate mutual compatibility on held-out test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    engine_test : MatchingEngine\n",
    "        The trained recommendation engine\n",
    "    test_data : cudf.DataFrame\n",
    "        Test set interactions\n",
    "    gender : str\n",
    "        'M' for males viewing females, 'F' for females viewing males\n",
    "    k : int\n",
    "        Number of recommendations to generate per user\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Evaluation results including metrics and recommendations\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import cudf\n",
    "    \n",
    "    print(f\"\\nüìä Evaluating MUTUAL compatibility - {gender} users viewing {'females' if gender=='M' else 'males'}...\\n\")\n",
    "    \n",
    "    # ‚ö° KEEP ON GPU: Filter likes on GPU\n",
    "    test_likes = test_data[test_data['like'] == 1][['decidermemberid', 'othermemberid']]\n",
    "    \n",
    "    # ‚ö° KEEP ON GPU: Get gender mapping on GPU\n",
    "    user_genders_df = engine_test.user_df[['user_id', 'gender']].rename(columns={'user_id': 'decidermemberid'})\n",
    "    \n",
    "    # ‚ö° KEEP ON GPU: Merge to get genders\n",
    "    test_likes_with_gender = test_likes.merge(user_genders_df, on='decidermemberid', how='left')\n",
    "    \n",
    "    # Build gender-specific like dictionaries\n",
    "    if gender == 'M':\n",
    "        # Males viewing females\n",
    "        male_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'M']\n",
    "        my_likes = male_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        my_likes = {k: set(v) for k, v in my_likes.items()}\n",
    "        \n",
    "        # Females who liked males (for mutual check)\n",
    "        female_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'F']\n",
    "        their_likes = female_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        their_likes = {k: set(v) for k, v in their_likes.items()}\n",
    "        \n",
    "        label = \"MALES VIEWING FEMALES\"\n",
    "        opposite_label = \"female\"\n",
    "    else:\n",
    "        # Females viewing males\n",
    "        female_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'F']\n",
    "        my_likes = female_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        my_likes = {k: set(v) for k, v in my_likes.items()}\n",
    "        \n",
    "        # Males who liked females (for mutual check)\n",
    "        male_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'M']\n",
    "        their_likes = male_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        their_likes = {k: set(v) for k, v in their_likes.items()}\n",
    "        \n",
    "        label = \"FEMALES VIEWING MALES\"\n",
    "        opposite_label = \"male\"\n",
    "    \n",
    "    # Calculate total mutual matches in test set\n",
    "    total_mutual_matches_in_test = 0\n",
    "    for user_id in my_likes:\n",
    "        for other_id in my_likes[user_id]:\n",
    "            # Check if mutual\n",
    "            if user_id in their_likes.get(other_id, set()):\n",
    "                total_mutual_matches_in_test += 1\n",
    "    \n",
    "    # ‚ö° KEEP ON GPU: Filter valid users on GPU\n",
    "    user_df_test = engine_test.user_df\n",
    "    valid_users_gpu = user_df_test[user_df_test['user_id'].isin(list(my_likes.keys()))]\n",
    "    test_users_valid = valid_users_gpu['user_id'].to_arrow().to_pylist()\n",
    "    \n",
    "    print(f\"Test users ({gender}) with held-out likes: {len(test_users_valid):,}\")\n",
    "    print(f\"Opposite gender users who liked someone: {len(their_likes):,}\")\n",
    "    print(f\"Total mutual matches in test set: {total_mutual_matches_in_test:,}\")\n",
    "    \n",
    "    if len(test_users_valid) == 0:\n",
    "        print(\"‚ö†Ô∏è No test users found\")\n",
    "        return None\n",
    "    \n",
    "    # Generate recommendations for ALL users\n",
    "    print(f\"Generating recommendations for all {len(test_users_valid):,} users...\")\n",
    "    recs_batch = engine_test.recommend_batch(test_users_valid, k=k)\n",
    "    \n",
    "    hits = 0\n",
    "    mutual_hits = 0\n",
    "    all_recs = []\n",
    "    mutual_matches = []\n",
    "    \n",
    "    for user_id in tqdm(test_users_valid, desc=f\"Evaluating {gender}\"):\n",
    "        # Extract user IDs from recommendations\n",
    "        recs = [rec[0] for rec in recs_batch[user_id]]\n",
    "        all_recs.extend(recs)\n",
    "        \n",
    "        # Get who this user liked in test set\n",
    "        actual_likes = my_likes.get(user_id, set())\n",
    "        recommended = set(recs)\n",
    "        \n",
    "        # One-sided hit (user liked someone we recommended)\n",
    "        if len(actual_likes & recommended) > 0:\n",
    "            hits += 1\n",
    "            \n",
    "            # Check for MUTUAL compatibility\n",
    "            for other_id in (actual_likes & recommended):\n",
    "                # Did the other person ALSO like this user in the test set?\n",
    "                if user_id in their_likes.get(other_id, set()):\n",
    "                    mutual_hits += 1\n",
    "                    mutual_matches.append((user_id, other_id))\n",
    "                    break  # Count once per user\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hit_rate = hits / len(test_users_valid)\n",
    "    mutual_hit_rate = mutual_hits / len(test_users_valid)\n",
    "    personalization = len(set(all_recs)) / len(all_recs) if len(all_recs) > 0 else 0\n",
    "    recall_of_matches = mutual_hits / total_mutual_matches_in_test if total_mutual_matches_in_test > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä HELD-OUT TEST SET EVALUATION (k={k}) - {label}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"One-Sided Hit Rate:    {hit_rate:.2%} ({hits:,}/{len(test_users_valid):,} users)\")\n",
    "    print(f\"   ‚Ü≥ User liked someone we recommended\")\n",
    "    print(f\"\\nMUTUAL Match Rate:     {mutual_hit_rate:.2%} ({mutual_hits:,}/{len(test_users_valid):,} users)\")\n",
    "    print(f\"   ‚Ü≥ Both users liked each other (TRUE compatibility!)\")\n",
    "    print(f\"\\nMatch Recall:          {recall_of_matches:.2%} ({mutual_hits:,}/{total_mutual_matches_in_test:,} matches)\")\n",
    "    print(f\"   ‚Ü≥ Proportion of ALL mutual matches we found in top-{k}\")\n",
    "    print(f\"\\nPersonalization:       {personalization:.2%}\")\n",
    "    print(f\"Unique recs:           {len(set(all_recs)):,}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if mutual_hits > 0:\n",
    "        print(f\"\\n‚úÖ Found {mutual_hits:,} mutual matches out of {total_mutual_matches_in_test:,} total in test set!\")\n",
    "        print(f\"üí° Match recall of {recall_of_matches:.1%} means we found {recall_of_matches:.1%} of all possible matches in top-{k}!\")\n",
    "    \n",
    "    return {\n",
    "        'gender': gender,\n",
    "        'test_users_valid': test_users_valid,\n",
    "        'all_recs': all_recs,\n",
    "        'hits': hits,\n",
    "        'mutual_hits': mutual_hits,\n",
    "        'mutual_matches': mutual_matches,\n",
    "        'hit_rate': hit_rate,\n",
    "        'mutual_hit_rate': mutual_hit_rate,\n",
    "        'personalization': personalization,\n",
    "        'total_mutual_matches_in_test': total_mutual_matches_in_test,\n",
    "        'recall_of_matches': recall_of_matches\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d39a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_recommendation_coverage(engine, results_dict, opposite_gender='F'):\n",
    "    \"\"\"\n",
    "    Analyse which users are being recommended and correlate with popularity/engagement.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    engine : MatchingEngine\n",
    "        The trained recommendation engine\n",
    "    results_dict : dict\n",
    "        Results from evaluate_mutual_compatibility function\n",
    "    opposite_gender : str\n",
    "        'F' to analyse females being recommended (to males)\n",
    "        'M' to analyse males being recommended (to females)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    \n",
    "    all_recs = results_dict['all_recs']\n",
    "    test_users_valid = results_dict['test_users_valid']\n",
    "    \n",
    "    # Get all potential candidates of specified gender\n",
    "    user_df_test = engine.user_df\n",
    "    candidates = user_df_test[user_df_test.gender == opposite_gender].copy().to_pandas()\n",
    "    \n",
    "    # Count how many times each candidate was recommended\n",
    "    rec_counts_dict = Counter(all_recs)\n",
    "    candidates['times_recommended'] = candidates['user_id'].map(lambda x: rec_counts_dict.get(x, 0))\n",
    "    \n",
    "    # Get candidates who were NEVER recommended\n",
    "    never_recommended = candidates[candidates['times_recommended'] == 0]\n",
    "    recommended_users = candidates[candidates['times_recommended'] > 0]\n",
    "    \n",
    "    gender_label = \"Female\" if opposite_gender == 'F' else \"Male\"\n",
    "    \n",
    "    print(f\"\\nüìä RECOMMENDATION COVERAGE ANALYSIS ({gender_label} Users)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total {gender_label.lower()} users available: {len(candidates):,}\")\n",
    "    print(f\"{gender_label}s recommended at least once: {len(recommended_users):,}\")\n",
    "    print(f\"{gender_label}s NEVER recommended: {len(never_recommended):,} ({len(never_recommended)/len(candidates)*100:.1f}%)\")\n",
    "    \n",
    "    # Check if any users were actually recommended\n",
    "    if len(recommended_users) == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: No {gender_label.lower()}s were recommended in this evaluation!\")\n",
    "        return candidates, recommended_users, never_recommended\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create scatter plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Times Recommended vs elo_rating (Popularity)\n",
    "    axes[0].scatter(candidates['elo_rating'], candidates['times_recommended'], \n",
    "                    alpha=0.5, s=20, c=candidates['times_recommended'], cmap='viridis')\n",
    "    axes[0].set_xlabel('elo_rating Score (Popularity)', fontsize=12)\n",
    "    axes[0].set_ylabel('Times Recommended', fontsize=12)\n",
    "    axes[0].set_title(f'{gender_label} Recommendation Frequency vs Popularity Score', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xscale('log')\n",
    "    \n",
    "    # Add correlation\n",
    "    valid_mask = ~candidates['elo_rating'].isna() & ~candidates['times_recommended'].isna()\n",
    "    if valid_mask.sum() > 0:\n",
    "        pearson_corr, _ = pearsonr(candidates[valid_mask]['elo_rating'], \n",
    "                                    candidates[valid_mask]['times_recommended'])\n",
    "        spearman_corr, _ = spearmanr(candidates[valid_mask]['elo_rating'], \n",
    "                                      candidates[valid_mask]['times_recommended'])\n",
    "        axes[0].text(0.05, 0.95, f'Pearson: {pearson_corr:.3f}\\nSpearman: {spearman_corr:.3f}', \n",
    "                    transform=axes[0].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 2. Times Recommended vs Engagement Score\n",
    "    axes[1].scatter(candidates['engagement_score'], candidates['times_recommended'], \n",
    "                    alpha=0.5, s=20, c=candidates['times_recommended'], cmap='plasma')\n",
    "    axes[1].set_xlabel('Engagement Score', fontsize=12)\n",
    "    axes[1].set_ylabel('Times Recommended', fontsize=12)\n",
    "    axes[1].set_title(f'{gender_label} Recommendation Frequency vs Engagement Score', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation\n",
    "    valid_mask = ~candidates['engagement_score'].isna() & ~candidates['times_recommended'].isna()\n",
    "    if valid_mask.sum() > 0:\n",
    "        pearson_corr, _ = pearsonr(candidates[valid_mask]['engagement_score'], \n",
    "                                    candidates[valid_mask]['times_recommended'])\n",
    "        spearman_corr, _ = spearmanr(candidates[valid_mask]['engagement_score'], \n",
    "                                      candidates[valid_mask]['times_recommended'])\n",
    "        axes[1].text(0.05, 0.95, f'Pearson: {pearson_corr:.3f}\\nSpearman: {spearman_corr:.3f}', \n",
    "                    transform=axes[1].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'recommendation_coverage_{opposite_gender}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed stats on never-recommended users\n",
    "    if len(never_recommended) > 0:\n",
    "        print(f\"\\nüìâ NEVER-RECOMMENDED {gender_label.upper()}S ANALYSIS:\")\n",
    "        print(f\"   ‚Ä¢ Avg elo_rating: {never_recommended['elo_rating'].mean():.6f} (vs {candidates['elo_rating'].mean():.6f} overall)\")\n",
    "        print(f\"   ‚Ä¢ Avg Engagement: {never_recommended['engagement_score'].mean():.2f} (vs {candidates['engagement_score'].mean():.2f} overall)\")\n",
    "        print(f\"   ‚Ä¢ League distribution:\")\n",
    "        for league in ['Bronze', 'Silver', 'Gold', 'Platinum', 'Diamond']:\n",
    "            count = len(never_recommended[never_recommended['league'] == league])\n",
    "            pct = count / len(never_recommended) * 100 if len(never_recommended) > 0 else 0\n",
    "            print(f\"      - {league}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Compare recommended vs never-recommended users\n",
    "    print(f\"\\nüìä COMPARISON: Recommended vs Never-Recommended {gender_label}s\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n{'Metric':<25} {'Recommended':<20} {'Never Recommended':<20}\")\n",
    "    print(f\"{'-'*65}\")\n",
    "    print(f\"{'Count':<25} {len(recommended_users):<20} {len(never_recommended):<20}\")\n",
    "    print(f\"{'Avg elo_rating':<25} {recommended_users['elo_rating'].mean():<20.6f} {never_recommended['elo_rating'].mean():<20.6f}\")\n",
    "    print(f\"{'Median elo_rating':<25} {recommended_users['elo_rating'].median():<20.6f} {never_recommended['elo_rating'].median():<20.6f}\")\n",
    "    print(f\"{'Avg Engagement':<25} {recommended_users['engagement_score'].mean():<20.2f} {never_recommended['engagement_score'].mean():<20.2f}\")\n",
    "    print(f\"{'Median Engagement':<25} {recommended_users['engagement_score'].median():<20.2f} {never_recommended['engagement_score'].median():<20.2f}\")\n",
    "    \n",
    "    return candidates, recommended_users, never_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe92774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ALL males viewing females (no sampling)\n",
    "results_males = evaluate_mutual_compatibility(engine_test, test_data, gender='M', k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8085dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate females viewing males\n",
    "results_females = evaluate_mutual_compatibility(engine_test, test_data, gender='F', k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee33a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99df6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case_statistics(target_id: int,\n",
    "                         interactions: \"cudf.DataFrame\",\n",
    "                         engine,\n",
    "                         rec_k: int = 30) -> None:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"### Test case statistics for user {target_id} ###\\n\")\n",
    "\n",
    "    decider_df = interactions[interactions[\"decidermemberid\"] == target_id]\n",
    "    total_decisions = len(decider_df)\n",
    "    like_rate = float(decider_df[\"like\"].mean()) if total_decisions else 0.0\n",
    "\n",
    "    likes_given = decider_df[decider_df[\"like\"] == 1]\n",
    "    likes_received = interactions[\n",
    "        (interactions[\"othermemberid\"] == target_id) & (interactions[\"like\"] == 1)\n",
    "    ]\n",
    "\n",
    "    matches = likes_given.merge(\n",
    "        likes_received,\n",
    "        left_on=\"othermemberid\",\n",
    "        right_on=\"decidermemberid\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    match_rate = len(matches) / total_decisions if total_decisions else 0.0\n",
    "    match_rate_given_likes = len(matches) / len(likes_given) if len(likes_given) else 0.0\n",
    "\n",
    "    print(f\"Like rate: {like_rate:.2%}\")\n",
    "    print(f\"Match rate: {match_rate:.2%}\")\n",
    "    print(f\"Match rate given likes: {match_rate_given_likes:.2%}\\n\")\n",
    "\n",
    "    recs = engine.recommend_batch([target_id], k=rec_k).get(target_id, [])\n",
    "    if not recs:\n",
    "        print(\"No recommendations available.\\n\")\n",
    "        return\n",
    "\n",
    "    rec_df = pd.DataFrame(recs, columns=[\"candidate_id\", \"score\"])\n",
    "    liked_ids = set(likes_given[\"othermemberid\"].to_pandas().tolist())\n",
    "    rec_df[\"label\"] = rec_df[\"candidate_id\"].apply(lambda cid: 1 if cid in liked_ids else 0)\n",
    "    \n",
    "    # üîç DIAGNOSTIC INFO\n",
    "    print(f\"DEBUG INFO:\")\n",
    "    print(f\"  Total people user liked in dataset: {len(liked_ids)}\")\n",
    "    print(f\"  People with label=1 in top-{rec_k}: {rec_df['label'].sum()}\")\n",
    "    print(f\"  Hit rate in top-{rec_k}: {rec_df['label'].sum() / len(liked_ids) * 100:.1f}%\\n\")\n",
    "    \n",
    "    print(rec_df.nlargest(5, \"score\"), end=\"\\n\\n\")\n",
    "\n",
    "    dcg = 0.0\n",
    "    for rank, rel in enumerate(rec_df.nlargest(rec_k, \"score\")[\"label\"], start=1):\n",
    "        dcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ideal_labels = rec_df.nlargest(rec_k, \"label\")[\"label\"]\n",
    "    idcg = 0.0\n",
    "    for rank, rel in enumerate(ideal_labels, start=1):\n",
    "        idcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ndcg_30 = dcg / idcg if idcg > 0 else 0.0\n",
    "    print(f\"NDCG@{rec_k} for user {target_id}: {ndcg_30:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_statistics(1142425, test_data, engine_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_statistics(336132, test_data, engine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3637cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b6ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def summarize_classification_pairs(pairs_df):\n",
    "    \"\"\"Add calibrated probabilities, print metrics, and return summary stats.\"\"\"\n",
    "    if pairs_df is None or len(pairs_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No pairs generated!\")\n",
    "        return None, {}\n",
    "\n",
    "    pairs_df = pairs_df.copy()\n",
    "    scores = pairs_df['score'].values\n",
    "    z = (scores - scores.mean()) / (scores.std() + 1e-12)  # standardize\n",
    "    pairs_df['proba'] = expit(z)  # sigmoid -> (0, 1)\n",
    "\n",
    "    metrics = {\n",
    "        'num_pairs': int(len(pairs_df)),\n",
    "        'positive_pairs': int(pairs_df['label'].sum()),\n",
    "        'negative_pairs': int((1 - pairs_df['label']).sum()),\n",
    "        'positive_rate': float(pairs_df['label'].mean()),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        metrics['auc_micro'] = float(roc_auc_score(pairs_df['label'], pairs_df['score']))\n",
    "        metrics['ap_micro'] = float(average_precision_score(pairs_df['label'], pairs_df['score']))\n",
    "        metrics['brier'] = float(brier_score_loss(pairs_df['label'], pairs_df['proba']))\n",
    "        metrics['logloss'] = float(log_loss(pairs_df['label'], pairs_df['proba']))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error computing metrics: {e}\")\n",
    "        print(f\"Label distribution: {pairs_df['label'].value_counts()}\")\n",
    "        return pairs_df, metrics\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total pairs evaluated:             {metrics['num_pairs']:,}\")\n",
    "    print(f\"Positive pairs (likes):            {metrics['positive_pairs']:,} ({metrics['positive_rate']:.2%})\")\n",
    "    print(f\"Negative pairs (no like):          {metrics['negative_pairs']:,}\")\n",
    "    print(f\"\\nMicro ROC AUC:                     {metrics['auc_micro']:.4f}\")\n",
    "    print(f\"Micro Average Precision (PR-AUC):  {metrics['ap_micro']:.4f}\")\n",
    "    print(f\"Brier score:                       {metrics['brier']:.4f}\")\n",
    "    print(f\"Log loss:                          {metrics['logloss']:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return pairs_df, metrics\n",
    "\n",
    "def evaluate_classification_metrics(engine, test_data, sample_size=None, k=100):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics (AUC, AP, Brier, Log Loss) on test set.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    engine : MatchingEngine\n",
    "        The trained recommendation engine\n",
    "    test_data : cudf.DataFrame\n",
    "        Test set interactions\n",
    "    sample_size : int, optional\n",
    "        Number of users to sample for evaluation (None = all users)\n",
    "    k : int, optional\n",
    "        Number of recommendations generated per user\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame | None, dict]: pairs with labels/scores and summary metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä CLASSIFICATION METRICS EVALUATION ON TEST SET\\n\")\n",
    "\n",
    "    test_users = test_data['decidermemberid'].unique().to_pandas().tolist()\n",
    "\n",
    "    if sample_size and sample_size < len(test_users):\n",
    "        test_users = np.random.choice(test_users, size=sample_size, replace=False).tolist()\n",
    "        print(f\"Sampled {sample_size:,} users from test set\")\n",
    "    else:\n",
    "        print(f\"Evaluating all {len(test_users):,} users in test set\")\n",
    "\n",
    "    test_likes = test_data[test_data['like'] == 1][['decidermemberid', 'othermemberid']]\n",
    "    ground_truth = test_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "    ground_truth = {k: set(v) for k, v in ground_truth.items()}\n",
    "\n",
    "    print(f\"Generating recommendations for {len(test_users):,} users...\")\n",
    "    recs_batch = engine.recommend_batch(test_users, k=k)\n",
    "\n",
    "    pairs_list = []\n",
    "    for user_id in tqdm(test_users, desc=\"Building pairs\"):\n",
    "        recs = recs_batch.get(user_id, [])\n",
    "        for candidate_id, score in recs:\n",
    "            label = 1 if candidate_id in ground_truth.get(user_id, set()) else 0\n",
    "            pairs_list.append({\n",
    "                'user': user_id,\n",
    "                'item': candidate_id,\n",
    "                'score': score,\n",
    "                'label': label\n",
    "            })\n",
    "\n",
    "    if not pairs_list:\n",
    "        print(\"‚ö†Ô∏è No pairs generated!\")\n",
    "        return None, {}\n",
    "\n",
    "    pairs_df = pd.DataFrame(pairs_list)\n",
    "    return summarize_classification_pairs(pairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions_df = (\n",
    "    test_data[['decidermemberid', 'othermemberid']]\n",
    "    .rename(columns={'decidermemberid': 'user', 'othermemberid': 'item'})\n",
    "    .drop_duplicates()\n",
    "    .to_pandas()\n",
    " )\n",
    "\n",
    "likes_by_user = (\n",
    "    test_data[test_data['like'] == 1][['decidermemberid', 'othermemberid']]\n",
    "    .rename(columns={'decidermemberid': 'user', 'othermemberid': 'item'})\n",
    "    .groupby('user')['item']\n",
    "    .agg(list)\n",
    "    .to_pandas()\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    " )\n",
    "\n",
    "def evaluate_classification_metrics_seen_only(pairs_df):\n",
    "    \"\"\"Restrict evaluation to impressions the user actually saw under the logged policy.\"\"\"\n",
    "    if pairs_df is None or len(pairs_df) == 0:\n",
    "        print('‚ö†Ô∏è No recommendation pairs supplied.')\n",
    "        return None, {}\n",
    "\n",
    "    filtered_pairs = pairs_df.merge(impressions_df, on=['user', 'item'], how='inner')\n",
    "\n",
    "    if filtered_pairs.empty:\n",
    "        print('‚ö†Ô∏è No recommendations overlapped with held-out impressions.')\n",
    "        return None, {}\n",
    "\n",
    "    return summarize_classification_pairs(filtered_pairs)\n",
    "\n",
    "def matched_hits_at_k(pairs_df, k=100):\n",
    "    \"\"\"Compute off-policy Matched Hits@K (precision-style metric) using only seen impressions.\"\"\"\n",
    "    if pairs_df is None or len(pairs_df) == 0:\n",
    "        print('‚ö†Ô∏è No recommendation pairs supplied.')\n",
    "        return 0.0, pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    ranked = pairs_df.sort_values(['user', 'score'], ascending=[True, False])\n",
    "    topk = ranked.groupby('user', as_index=False).head(k)\n",
    "\n",
    "    if topk.empty:\n",
    "        print('‚ö†Ô∏è No recommendations available for matched hits calculation.')\n",
    "        return 0.0, topk, pd.DataFrame()\n",
    "\n",
    "    seen_topk = topk.merge(impressions_df, on=['user', 'item'], how='inner')\n",
    "\n",
    "    if seen_topk.empty:\n",
    "        print('‚ö†Ô∏è None of the recommended pairs were observed under the logged policy.')\n",
    "        return 0.0, seen_topk, pd.DataFrame()\n",
    "\n",
    "    seen_topk['matched_like'] = seen_topk.apply(\n",
    "        lambda row: 1 if row['item'] in likes_by_user.get(row['user'], set()) else 0, axis=1\n",
    "    )\n",
    "\n",
    "    per_user = (\n",
    "        seen_topk.groupby('user')['matched_like']\n",
    "        .agg(['sum', 'count'])\n",
    "        .rename(columns={'sum': 'matched_likes', 'count': 'items_returned'})\n",
    "    )\n",
    "\n",
    "    num_users = len(per_user)\n",
    "    if num_users == 0:\n",
    "        print('‚ö†Ô∏è No users with seen recommendations for matched hits calculation.')\n",
    "        return 0.0, seen_topk, per_user\n",
    "\n",
    "    total_items_returned = per_user['items_returned'].sum()\n",
    "    matched_hits = per_user['matched_likes'].sum() / total_items_returned if total_items_returned else 0.0\n",
    "    per_user['matched_hits_at_k'] = per_user['matched_likes'] / per_user['items_returned']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"### OFF-POLICY MATCHED HITS@K (SEEN IMPRESSIONS) ###\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Users evaluated:                  {num_users:,}\")\n",
    "    print(f\"Total seen recommendations:       {total_items_returned:,}\")\n",
    "    print(f\"Matched Hits@{k} (micro):          {matched_hits:.4f}\")\n",
    "    print(f\"Median per-user matched hits:     {per_user['matched_hits_at_k'].median():.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return matched_hits, seen_topk, per_user\n",
    "\n",
    "def logged_policy_matched_hits(interactions):\n",
    "    \"\"\"Compute matched hit statistics for the historical logged policy.\"\"\"\n",
    "    df = interactions[['decidermemberid', 'othermemberid', 'like']].to_pandas()\n",
    "\n",
    "    if df.empty:\n",
    "        print('‚ö†Ô∏è Interaction data is empty.')\n",
    "        return 0.0, pd.DataFrame()\n",
    "\n",
    "    per_user = (\n",
    "        df.groupby('decidermemberid')\n",
    "        .agg(impressions=('othermemberid', 'count'), matched_likes=('like', 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    per_user['matched_hits_rate'] = per_user['matched_likes'] / per_user['impressions']\n",
    "\n",
    "    total_impressions = per_user['impressions'].sum()\n",
    "    total_matched_likes = per_user['matched_likes'].sum()\n",
    "\n",
    "    micro_rate = total_matched_likes / total_impressions if total_impressions else 0.0\n",
    "    median_rate = per_user['matched_hits_rate'].median() if not per_user.empty else 0.0\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"### LOGGED POLICY MATCHED HITS ###\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Users evaluated:                  {len(per_user):,}\")\n",
    "    print(f\"Total logged impressions:         {total_impressions:,}\")\n",
    "    print(f\"Total logged likes (matches):     {int(total_matched_likes):,}\")\n",
    "    print(f\"Matched Hits (micro):             {micro_rate:.4f}\")\n",
    "    print(f\"Median per-user matched hits:     {median_rate:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return micro_rate, per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7bd2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä CLASSIFICATION METRICS EVALUATION ON TEST SET\n",
      "\n",
      "Evaluating all 30,327 users in test set\n",
      "Generating recommendations for 30,327 users...\n",
      "Generating recommendations for 30,327 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30327/30327 [00:01<00:00, 25059.66it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\n",
      "======================================================================\n",
      "Total pairs evaluated:             2,442,961\n",
      "Positive pairs (likes):            25,337 (1.04%)\n",
      "Negative pairs (no like):          2,417,624\n",
      "\n",
      "Micro ROC AUC:                     0.7418\n",
      "Micro Average Precision (PR-AUC):  0.0252\n",
      "Brier score:                       0.2530\n",
      "Log loss:                          0.7781\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\n",
      "======================================================================\n",
      "Total pairs evaluated:             47,645\n",
      "Positive pairs (likes):            25,337 (53.18%)\n",
      "Negative pairs (no like):          22,308\n",
      "\n",
      "Micro ROC AUC:                     0.7051\n",
      "Micro Average Precision (PR-AUC):  0.6945\n",
      "Brier score:                       0.2329\n",
      "Log loss:                          0.6655\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\n",
      "======================================================================\n",
      "Total pairs evaluated:             47,645\n",
      "Positive pairs (likes):            25,337 (53.18%)\n",
      "Negative pairs (no like):          22,308\n",
      "\n",
      "Micro ROC AUC:                     0.7051\n",
      "Micro Average Precision (PR-AUC):  0.6945\n",
      "Brier score:                       0.2329\n",
      "Log loss:                          0.6655\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### OFF-POLICY MATCHED HITS@K (SEEN IMPRESSIONS) ###\n",
      "======================================================================\n",
      "Users evaluated:                  13,154\n",
      "Total seen recommendations:       47,645\n",
      "Matched Hits@100 (micro):          0.5318\n",
      "Median per-user matched hits:     0.5000\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### OFF-POLICY MATCHED HITS@K (SEEN IMPRESSIONS) ###\n",
      "======================================================================\n",
      "Users evaluated:                  13,154\n",
      "Total seen recommendations:       47,645\n",
      "Matched Hits@100 (micro):          0.5318\n",
      "Median per-user matched hits:     0.5000\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate the raw recommendation pairs\n",
    "pairs_df, overall_metrics = evaluate_classification_metrics(engine_test, test_data, sample_size=40000, k=100)\n",
    "\n",
    "print()\n",
    "\n",
    "# re-score using only impressions the user actually saw\n",
    "seen_pairs_df, seen_metrics = evaluate_classification_metrics_seen_only(pairs_df)\n",
    "\n",
    "print()\n",
    "\n",
    "# compute matched hits@K (off-policy precision style metric)\n",
    "matched_hits, topk_matched_pairs, per_user_matched = matched_hits_at_k(pairs_df, k=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchmaker-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
