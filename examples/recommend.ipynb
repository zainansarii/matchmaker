{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3592a605",
   "metadata": {},
   "source": [
    "## ⚠️ Proper Evaluation with Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2291ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zain/anaconda3/envs/matchmaker-dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original interaction data...\n",
      "Train set: 7,862,310 interactions\n",
      "Test set:  1,965,578 interactions\n",
      "Train set: 7,862,310 interactions\n",
      "Test set:  1,965,578 interactions\n",
      "\n",
      "🔄 Training model on 80% of data...\n",
      "Reading data... \n",
      "🔄 Training model on 80% of data...\n",
      "Reading data... ✅\n",
      "Fitting ALS... \n",
      "🚀 Preparing data...\n",
      "✅\n",
      "Fitting ALS... \n",
      "🚀 Preparing data...\n",
      "🎯 Training male→female ALS...\n",
      "🎯 Training male→female ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 16.93it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Training female→male ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 309.95it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Converting factors to CuPy arrays...\n",
      "✅ Trained M2F ALS with 31134 males × 32994 females\n",
      "✅ Trained F2M ALS with 9925 females × 38446 males\n",
      "Complete! ✅\n",
      "User DF updated ✅\n",
      "User DF updated ✅\n",
      "User DF updated ✅\n",
      "Building FAISS recommender (pop)... User DF updated ✅\n",
      "Building FAISS recommender (pop)... ✅\n",
      "✅ Training complete on train set\n",
      "✅\n",
      "✅ Training complete on train set\n"
     ]
    }
   ],
   "source": [
    "# Proper train/test split evaluation\n",
    "from matchmaker import MatchingEngine\n",
    "import cudf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load original data and split by timestamp\n",
    "print(\"Loading original interaction data...\")\n",
    "raw_data = cudf.read_csv(\"data/swipes_clean.csv\")\n",
    "\n",
    "# Sort by timestamp and split 80/20\n",
    "raw_data = raw_data.sort_values('timestamp')\n",
    "split_idx = int(len(raw_data) * 0.8)\n",
    "\n",
    "train_data = raw_data.iloc[:split_idx]\n",
    "test_data = raw_data.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train set: {len(train_data):,} interactions\")\n",
    "print(f\"Test set:  {len(test_data):,} interactions\")\n",
    "\n",
    "# Save splits temporarily\n",
    "train_data.to_csv(\"/tmp/train_split.csv\", index=False)\n",
    "test_data.to_csv(\"/tmp/test_split.csv\", index=False)\n",
    "\n",
    "# 2. Build a NEW engine on ONLY the training data\n",
    "print(\"\\n🔄 Training model on 80% of data...\")\n",
    "engine_test = MatchingEngine()\n",
    "engine_test.load_interactions(\"/tmp/train_split.csv\",\n",
    "    decider_col='decidermemberid',\n",
    "    other_col='othermemberid', \n",
    "    like_col='like', \n",
    "    timestamp_col='timestamp',\n",
    "    gender_col='decidergender')\n",
    "engine_test.run_engagement()\n",
    "engine_test.run_elo()\n",
    "engine_test.build_recommender()\n",
    "print(\"✅ Training complete on train set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e4c54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mutual_compatibility(engine_test, test_data, gender='M', k=100):\n",
    "    \"\"\"\n",
    "    Evaluate mutual compatibility on held-out test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    engine_test : MatchingEngine\n",
    "        The trained recommendation engine\n",
    "    test_data : cudf.DataFrame\n",
    "        Test set interactions\n",
    "    gender : str\n",
    "        'M' for males viewing females, 'F' for females viewing males\n",
    "    k : int\n",
    "        Number of recommendations to generate per user\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Evaluation results including metrics and recommendations\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    import cudf\n",
    "    \n",
    "    print(f\"\\n📊 Evaluating MUTUAL compatibility - {gender} users viewing {'females' if gender=='M' else 'males'}...\\n\")\n",
    "    \n",
    "    # ⚡ KEEP ON GPU: Filter likes on GPU\n",
    "    test_likes = test_data[test_data['like'] == 1][['decidermemberid', 'othermemberid']]\n",
    "    \n",
    "    # ⚡ KEEP ON GPU: Get gender mapping on GPU\n",
    "    user_genders_df = engine_test.user_df[['user_id', 'gender']].rename(columns={'user_id': 'decidermemberid'})\n",
    "    \n",
    "    # ⚡ KEEP ON GPU: Merge to get genders\n",
    "    test_likes_with_gender = test_likes.merge(user_genders_df, on='decidermemberid', how='left')\n",
    "    \n",
    "    # Build gender-specific like dictionaries\n",
    "    if gender == 'M':\n",
    "        # Males viewing females\n",
    "        male_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'M']\n",
    "        my_likes = male_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        my_likes = {k: set(v) for k, v in my_likes.items()}\n",
    "        \n",
    "        # Females who liked males (for mutual check)\n",
    "        female_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'F']\n",
    "        their_likes = female_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        their_likes = {k: set(v) for k, v in their_likes.items()}\n",
    "        \n",
    "        label = \"MALES VIEWING FEMALES\"\n",
    "        opposite_label = \"female\"\n",
    "    else:\n",
    "        # Females viewing males\n",
    "        female_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'F']\n",
    "        my_likes = female_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        my_likes = {k: set(v) for k, v in my_likes.items()}\n",
    "        \n",
    "        # Males who liked females (for mutual check)\n",
    "        male_likes = test_likes_with_gender[test_likes_with_gender['gender'] == 'M']\n",
    "        their_likes = male_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "        their_likes = {k: set(v) for k, v in their_likes.items()}\n",
    "        \n",
    "        label = \"FEMALES VIEWING MALES\"\n",
    "        opposite_label = \"male\"\n",
    "    \n",
    "    # Calculate total mutual matches in test set\n",
    "    total_mutual_matches_in_test = 0\n",
    "    for user_id in my_likes:\n",
    "        for other_id in my_likes[user_id]:\n",
    "            # Check if mutual\n",
    "            if user_id in their_likes.get(other_id, set()):\n",
    "                total_mutual_matches_in_test += 1\n",
    "    \n",
    "    # ⚡ KEEP ON GPU: Filter valid users on GPU\n",
    "    user_df_test = engine_test.user_df\n",
    "    valid_users_gpu = user_df_test[user_df_test['user_id'].isin(list(my_likes.keys()))]\n",
    "    test_users_valid = valid_users_gpu['user_id'].to_arrow().to_pylist()\n",
    "    \n",
    "    print(f\"Test users ({gender}) with held-out likes: {len(test_users_valid):,}\")\n",
    "    print(f\"Opposite gender users who liked someone: {len(their_likes):,}\")\n",
    "    print(f\"Total mutual matches in test set: {total_mutual_matches_in_test:,}\")\n",
    "    \n",
    "    if len(test_users_valid) == 0:\n",
    "        print(\"⚠️ No test users found\")\n",
    "        return None\n",
    "    \n",
    "    # Generate recommendations for ALL users\n",
    "    print(f\"Generating recommendations for all {len(test_users_valid):,} users...\")\n",
    "    recs_batch = engine_test.recommend_batch(test_users_valid, k=k)\n",
    "    \n",
    "    hits = 0\n",
    "    mutual_hits = 0\n",
    "    all_recs = []\n",
    "    mutual_matches = []\n",
    "    \n",
    "    for user_id in tqdm(test_users_valid, desc=f\"Evaluating {gender}\"):\n",
    "        # Extract user IDs from recommendations\n",
    "        recs = [rec[0] for rec in recs_batch[user_id]]\n",
    "        all_recs.extend(recs)\n",
    "        \n",
    "        # Get who this user liked in test set\n",
    "        actual_likes = my_likes.get(user_id, set())\n",
    "        recommended = set(recs)\n",
    "        \n",
    "        # One-sided hit (user liked someone we recommended)\n",
    "        if len(actual_likes & recommended) > 0:\n",
    "            hits += 1\n",
    "            \n",
    "            # Check for MUTUAL compatibility\n",
    "            for other_id in (actual_likes & recommended):\n",
    "                # Did the other person ALSO like this user in the test set?\n",
    "                if user_id in their_likes.get(other_id, set()):\n",
    "                    mutual_hits += 1\n",
    "                    mutual_matches.append((user_id, other_id))\n",
    "                    break  # Count once per user\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hit_rate = hits / len(test_users_valid)\n",
    "    mutual_hit_rate = mutual_hits / len(test_users_valid)\n",
    "    personalization = len(set(all_recs)) / len(all_recs) if len(all_recs) > 0 else 0\n",
    "    recall_of_matches = mutual_hits / total_mutual_matches_in_test if total_mutual_matches_in_test > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"📊 HELD-OUT TEST SET EVALUATION (k={k}) - {label}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"One-Sided Hit Rate:    {hit_rate:.2%} ({hits:,}/{len(test_users_valid):,} users)\")\n",
    "    print(f\"   ↳ User liked someone we recommended\")\n",
    "    print(f\"\\nMUTUAL Match Rate:     {mutual_hit_rate:.2%} ({mutual_hits:,}/{len(test_users_valid):,} users)\")\n",
    "    print(f\"   ↳ Both users liked each other (TRUE compatibility!)\")\n",
    "    print(f\"\\nMatch Recall:          {recall_of_matches:.2%} ({mutual_hits:,}/{total_mutual_matches_in_test:,} matches)\")\n",
    "    print(f\"   ↳ Proportion of ALL mutual matches we found in top-{k}\")\n",
    "    print(f\"\\nPersonalization:       {personalization:.2%}\")\n",
    "    print(f\"Unique recs:           {len(set(all_recs)):,}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if mutual_hits > 0:\n",
    "        print(f\"\\n✅ Found {mutual_hits:,} mutual matches out of {total_mutual_matches_in_test:,} total in test set!\")\n",
    "        print(f\"💡 Match recall of {recall_of_matches:.1%} means we found {recall_of_matches:.1%} of all possible matches in top-{k}!\")\n",
    "    \n",
    "    return {\n",
    "        'gender': gender,\n",
    "        'test_users_valid': test_users_valid,\n",
    "        'all_recs': all_recs,\n",
    "        'hits': hits,\n",
    "        'mutual_hits': mutual_hits,\n",
    "        'mutual_matches': mutual_matches,\n",
    "        'hit_rate': hit_rate,\n",
    "        'mutual_hit_rate': mutual_hit_rate,\n",
    "        'personalization': personalization,\n",
    "        'total_mutual_matches_in_test': total_mutual_matches_in_test,\n",
    "        'recall_of_matches': recall_of_matches\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d39a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_recommendation_coverage(engine, results_dict, opposite_gender='F'):\n",
    "    \"\"\"\n",
    "    Analyse which users are being recommended and correlate with popularity/engagement.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    engine : MatchingEngine\n",
    "        The trained recommendation engine\n",
    "    results_dict : dict\n",
    "        Results from evaluate_mutual_compatibility function\n",
    "    opposite_gender : str\n",
    "        'F' to analyse females being recommended (to males)\n",
    "        'M' to analyse males being recommended (to females)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    \n",
    "    all_recs = results_dict['all_recs']\n",
    "    test_users_valid = results_dict['test_users_valid']\n",
    "    \n",
    "    # Get all potential candidates of specified gender\n",
    "    user_df_test = engine.user_df\n",
    "    candidates = user_df_test[user_df_test.gender == opposite_gender].copy().to_pandas()\n",
    "    \n",
    "    # Count how many times each candidate was recommended\n",
    "    rec_counts_dict = Counter(all_recs)\n",
    "    candidates['times_recommended'] = candidates['user_id'].map(lambda x: rec_counts_dict.get(x, 0))\n",
    "    \n",
    "    # Get candidates who were NEVER recommended\n",
    "    never_recommended = candidates[candidates['times_recommended'] == 0]\n",
    "    recommended_users = candidates[candidates['times_recommended'] > 0]\n",
    "    \n",
    "    gender_label = \"Female\" if opposite_gender == 'F' else \"Male\"\n",
    "    \n",
    "    print(f\"\\n📊 RECOMMENDATION COVERAGE ANALYSIS ({gender_label} Users)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total {gender_label.lower()} users available: {len(candidates):,}\")\n",
    "    print(f\"{gender_label}s recommended at least once: {len(recommended_users):,}\")\n",
    "    print(f\"{gender_label}s NEVER recommended: {len(never_recommended):,} ({len(never_recommended)/len(candidates)*100:.1f}%)\")\n",
    "    \n",
    "    # Check if any users were actually recommended\n",
    "    if len(recommended_users) == 0:\n",
    "        print(f\"\\n⚠️  WARNING: No {gender_label.lower()}s were recommended in this evaluation!\")\n",
    "        return candidates, recommended_users, never_recommended\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create scatter plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Times Recommended vs elo_rating (Popularity)\n",
    "    axes[0].scatter(candidates['elo_rating'], candidates['times_recommended'], \n",
    "                    alpha=0.5, s=20, c=candidates['times_recommended'], cmap='viridis')\n",
    "    axes[0].set_xlabel('elo_rating Score (Popularity)', fontsize=12)\n",
    "    axes[0].set_ylabel('Times Recommended', fontsize=12)\n",
    "    axes[0].set_title(f'{gender_label} Recommendation Frequency vs Popularity Score', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xscale('log')\n",
    "    \n",
    "    # Add correlation\n",
    "    valid_mask = ~candidates['elo_rating'].isna() & ~candidates['times_recommended'].isna()\n",
    "    if valid_mask.sum() > 0:\n",
    "        pearson_corr, _ = pearsonr(candidates[valid_mask]['elo_rating'], \n",
    "                                    candidates[valid_mask]['times_recommended'])\n",
    "        spearman_corr, _ = spearmanr(candidates[valid_mask]['elo_rating'], \n",
    "                                      candidates[valid_mask]['times_recommended'])\n",
    "        axes[0].text(0.05, 0.95, f'Pearson: {pearson_corr:.3f}\\nSpearman: {spearman_corr:.3f}', \n",
    "                    transform=axes[0].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 2. Times Recommended vs Engagement Score\n",
    "    axes[1].scatter(candidates['engagement_score'], candidates['times_recommended'], \n",
    "                    alpha=0.5, s=20, c=candidates['times_recommended'], cmap='plasma')\n",
    "    axes[1].set_xlabel('Engagement Score', fontsize=12)\n",
    "    axes[1].set_ylabel('Times Recommended', fontsize=12)\n",
    "    axes[1].set_title(f'{gender_label} Recommendation Frequency vs Engagement Score', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation\n",
    "    valid_mask = ~candidates['engagement_score'].isna() & ~candidates['times_recommended'].isna()\n",
    "    if valid_mask.sum() > 0:\n",
    "        pearson_corr, _ = pearsonr(candidates[valid_mask]['engagement_score'], \n",
    "                                    candidates[valid_mask]['times_recommended'])\n",
    "        spearman_corr, _ = spearmanr(candidates[valid_mask]['engagement_score'], \n",
    "                                      candidates[valid_mask]['times_recommended'])\n",
    "        axes[1].text(0.05, 0.95, f'Pearson: {pearson_corr:.3f}\\nSpearman: {spearman_corr:.3f}', \n",
    "                    transform=axes[1].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'recommendation_coverage_{opposite_gender}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed stats on never-recommended users\n",
    "    if len(never_recommended) > 0:\n",
    "        print(f\"\\n📉 NEVER-RECOMMENDED {gender_label.upper()}S ANALYSIS:\")\n",
    "        print(f\"   • Avg elo_rating: {never_recommended['elo_rating'].mean():.6f} (vs {candidates['elo_rating'].mean():.6f} overall)\")\n",
    "        print(f\"   • Avg Engagement: {never_recommended['engagement_score'].mean():.2f} (vs {candidates['engagement_score'].mean():.2f} overall)\")\n",
    "        print(f\"   • League distribution:\")\n",
    "        for league in ['Bronze', 'Silver', 'Gold', 'Platinum', 'Diamond']:\n",
    "            count = len(never_recommended[never_recommended['league'] == league])\n",
    "            pct = count / len(never_recommended) * 100 if len(never_recommended) > 0 else 0\n",
    "            print(f\"      - {league}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Compare recommended vs never-recommended users\n",
    "    print(f\"\\n📊 COMPARISON: Recommended vs Never-Recommended {gender_label}s\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n{'Metric':<25} {'Recommended':<20} {'Never Recommended':<20}\")\n",
    "    print(f\"{'-'*65}\")\n",
    "    print(f\"{'Count':<25} {len(recommended_users):<20} {len(never_recommended):<20}\")\n",
    "    print(f\"{'Avg elo_rating':<25} {recommended_users['elo_rating'].mean():<20.6f} {never_recommended['elo_rating'].mean():<20.6f}\")\n",
    "    print(f\"{'Median elo_rating':<25} {recommended_users['elo_rating'].median():<20.6f} {never_recommended['elo_rating'].median():<20.6f}\")\n",
    "    print(f\"{'Avg Engagement':<25} {recommended_users['engagement_score'].mean():<20.2f} {never_recommended['engagement_score'].mean():<20.2f}\")\n",
    "    print(f\"{'Median Engagement':<25} {recommended_users['engagement_score'].median():<20.2f} {never_recommended['engagement_score'].median():<20.2f}\")\n",
    "    \n",
    "    return candidates, recommended_users, never_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe92774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ALL males viewing females (no sampling)\n",
    "results_males = evaluate_mutual_compatibility(engine_test, test_data, gender='M', k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8085dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate females viewing males\n",
    "results_females = evaluate_mutual_compatibility(engine_test, test_data, gender='F', k=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee33a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99df6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_case_statistics(target_id: int,\n",
    "                         interactions: \"cudf.DataFrame\",\n",
    "                         engine,\n",
    "                         rec_k: int = 30) -> None:\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"### Test case statistics for user {target_id} ###\\n\")\n",
    "\n",
    "    decider_df = interactions[interactions[\"decidermemberid\"] == target_id]\n",
    "    total_decisions = len(decider_df)\n",
    "    like_rate = float(decider_df[\"like\"].mean()) if total_decisions else 0.0\n",
    "\n",
    "    likes_given = decider_df[decider_df[\"like\"] == 1]\n",
    "    likes_received = interactions[\n",
    "        (interactions[\"othermemberid\"] == target_id) & (interactions[\"like\"] == 1)\n",
    "    ]\n",
    "\n",
    "    matches = likes_given.merge(\n",
    "        likes_received,\n",
    "        left_on=\"othermemberid\",\n",
    "        right_on=\"decidermemberid\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    match_rate = len(matches) / total_decisions if total_decisions else 0.0\n",
    "    match_rate_given_likes = len(matches) / len(likes_given) if len(likes_given) else 0.0\n",
    "\n",
    "    print(f\"Like rate: {like_rate:.2%}\")\n",
    "    print(f\"Match rate: {match_rate:.2%}\")\n",
    "    print(f\"Match rate given likes: {match_rate_given_likes:.2%}\\n\")\n",
    "\n",
    "    recs = engine.recommend_batch([target_id], k=rec_k).get(target_id, [])\n",
    "    if not recs:\n",
    "        print(\"No recommendations available.\\n\")\n",
    "        return\n",
    "\n",
    "    rec_df = pd.DataFrame(recs, columns=[\"candidate_id\", \"score\"])\n",
    "    liked_ids = set(likes_given[\"othermemberid\"].to_pandas().tolist())\n",
    "    rec_df[\"label\"] = rec_df[\"candidate_id\"].apply(lambda cid: 1 if cid in liked_ids else 0)\n",
    "    \n",
    "    # 🔍 DIAGNOSTIC INFO\n",
    "    print(f\"DEBUG INFO:\")\n",
    "    print(f\"  Total people user liked in dataset: {len(liked_ids)}\")\n",
    "    print(f\"  People with label=1 in top-{rec_k}: {rec_df['label'].sum()}\")\n",
    "    print(f\"  Hit rate in top-{rec_k}: {rec_df['label'].sum() / len(liked_ids) * 100:.1f}%\\n\")\n",
    "    \n",
    "    print(rec_df.nlargest(5, \"score\"), end=\"\\n\\n\")\n",
    "\n",
    "    dcg = 0.0\n",
    "    for rank, rel in enumerate(rec_df.nlargest(rec_k, \"score\")[\"label\"], start=1):\n",
    "        dcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ideal_labels = rec_df.nlargest(rec_k, \"label\")[\"label\"]\n",
    "    idcg = 0.0\n",
    "    for rank, rel in enumerate(ideal_labels, start=1):\n",
    "        idcg += (2 ** rel - 1) / np.log2(rank + 1)\n",
    "\n",
    "    ndcg_30 = dcg / idcg if idcg > 0 else 0.0\n",
    "    print(f\"NDCG@{rec_k} for user {target_id}: {ndcg_30:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_statistics(1142425, test_data, engine_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case_statistics(336132, test_data, engine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3637cb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b6ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def summarize_classification_pairs(pairs_df):\n",
    "    \"\"\"Add calibrated probabilities, print metrics, and return summary stats.\"\"\"\n",
    "    if pairs_df is None or len(pairs_df) == 0:\n",
    "        print(\"⚠️ No pairs generated!\")\n",
    "        return None, {}\n",
    "\n",
    "    pairs_df = pairs_df.copy()\n",
    "    scores = pairs_df['score'].values\n",
    "    z = (scores - scores.mean()) / (scores.std() + 1e-12)  # standardize\n",
    "    pairs_df['proba'] = expit(z)  # sigmoid -> (0, 1)\n",
    "\n",
    "    metrics = {\n",
    "        'num_pairs': int(len(pairs_df)),\n",
    "        'positive_pairs': int(pairs_df['label'].sum()),\n",
    "        'negative_pairs': int((1 - pairs_df['label']).sum()),\n",
    "        'positive_rate': float(pairs_df['label'].mean()),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        metrics['auc_micro'] = float(roc_auc_score(pairs_df['label'], pairs_df['score']))\n",
    "        metrics['ap_micro'] = float(average_precision_score(pairs_df['label'], pairs_df['score']))\n",
    "        metrics['brier'] = float(brier_score_loss(pairs_df['label'], pairs_df['proba']))\n",
    "        metrics['logloss'] = float(log_loss(pairs_df['label'], pairs_df['proba']))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error computing metrics: {e}\")\n",
    "        print(f\"Label distribution: {pairs_df['label'].value_counts()}\")\n",
    "        return pairs_df, metrics\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total pairs evaluated:             {metrics['num_pairs']:,}\")\n",
    "    print(f\"Positive pairs (likes):            {metrics['positive_pairs']:,} ({metrics['positive_rate']:.2%})\")\n",
    "    print(f\"Negative pairs (no like):          {metrics['negative_pairs']:,}\")\n",
    "    print(f\"\\nMicro ROC AUC:                     {metrics['auc_micro']:.4f}\")\n",
    "    print(f\"Micro Average Precision (PR-AUC):  {metrics['ap_micro']:.4f}\")\n",
    "    print(f\"Brier score:                       {metrics['brier']:.4f}\")\n",
    "    print(f\"Log loss:                          {metrics['logloss']:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return pairs_df, metrics\n",
    "\n",
    "def evaluate_classification_metrics(engine, test_data, sample_size=None, k=100):\n",
    "    \"\"\"\n",
    "    Evaluate classification metrics (AUC, AP, Brier, Log Loss) on test set.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    engine : MatchingEngine\n",
    "        The trained recommendation engine\n",
    "    test_data : cudf.DataFrame\n",
    "        Test set interactions\n",
    "    sample_size : int, optional\n",
    "        Number of users to sample for evaluation (None = all users)\n",
    "    k : int, optional\n",
    "        Number of recommendations generated per user\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pd.DataFrame | None, dict]: pairs with labels/scores and summary metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 CLASSIFICATION METRICS EVALUATION ON TEST SET\\n\")\n",
    "\n",
    "    test_users = test_data['decidermemberid'].unique().to_pandas().tolist()\n",
    "\n",
    "    if sample_size and sample_size < len(test_users):\n",
    "        test_users = np.random.choice(test_users, size=sample_size, replace=False).tolist()\n",
    "        print(f\"Sampled {sample_size:,} users from test set\")\n",
    "    else:\n",
    "        print(f\"Evaluating all {len(test_users):,} users in test set\")\n",
    "\n",
    "    test_likes = test_data[test_data['like'] == 1][['decidermemberid', 'othermemberid']]\n",
    "    ground_truth = test_likes.groupby('decidermemberid')['othermemberid'].agg(list).to_pandas()\n",
    "    ground_truth = {k: set(v) for k, v in ground_truth.items()}\n",
    "\n",
    "    print(f\"Generating recommendations for {len(test_users):,} users...\")\n",
    "    recs_batch = engine.recommend_batch(test_users, k=k)\n",
    "\n",
    "    pairs_list = []\n",
    "    for user_id in tqdm(test_users, desc=\"Building pairs\"):\n",
    "        recs = recs_batch.get(user_id, [])\n",
    "        for candidate_id, score in recs:\n",
    "            label = 1 if candidate_id in ground_truth.get(user_id, set()) else 0\n",
    "            pairs_list.append({\n",
    "                'user': user_id,\n",
    "                'item': candidate_id,\n",
    "                'score': score,\n",
    "                'label': label\n",
    "            })\n",
    "\n",
    "    if not pairs_list:\n",
    "        print(\"⚠️ No pairs generated!\")\n",
    "        return None, {}\n",
    "\n",
    "    pairs_df = pd.DataFrame(pairs_list)\n",
    "    return summarize_classification_pairs(pairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions_df = (\n",
    "    test_data[['decidermemberid', 'othermemberid']]\n",
    "    .rename(columns={'decidermemberid': 'user', 'othermemberid': 'item'})\n",
    "    .drop_duplicates()\n",
    "    .to_pandas()\n",
    " )\n",
    "\n",
    "likes_by_user = (\n",
    "    test_data[test_data['like'] == 1][['decidermemberid', 'othermemberid']]\n",
    "    .rename(columns={'decidermemberid': 'user', 'othermemberid': 'item'})\n",
    "    .groupby('user')['item']\n",
    "    .agg(list)\n",
    "    .to_pandas()\n",
    "    .apply(set)\n",
    "    .to_dict()\n",
    " )\n",
    "\n",
    "def evaluate_classification_metrics_seen_only(pairs_df):\n",
    "    \"\"\"Restrict evaluation to impressions the user actually saw under the logged policy.\"\"\"\n",
    "    if pairs_df is None or len(pairs_df) == 0:\n",
    "        print('⚠️ No recommendation pairs supplied.')\n",
    "        return None, {}\n",
    "\n",
    "    filtered_pairs = pairs_df.merge(impressions_df, on=['user', 'item'], how='inner')\n",
    "\n",
    "    if filtered_pairs.empty:\n",
    "        print('⚠️ No recommendations overlapped with held-out impressions.')\n",
    "        return None, {}\n",
    "\n",
    "    return summarize_classification_pairs(filtered_pairs)\n",
    "\n",
    "def matched_hits_at_k(pairs_df, k=100):\n",
    "    \"\"\"Compute off-policy Matched Hits@K (precision-style metric) using only seen impressions.\"\"\"\n",
    "    if pairs_df is None or len(pairs_df) == 0:\n",
    "        print('⚠️ No recommendation pairs supplied.')\n",
    "        return 0.0, pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    ranked = pairs_df.sort_values(['user', 'score'], ascending=[True, False])\n",
    "    topk = ranked.groupby('user', as_index=False).head(k)\n",
    "\n",
    "    if topk.empty:\n",
    "        print('⚠️ No recommendations available for matched hits calculation.')\n",
    "        return 0.0, topk, pd.DataFrame()\n",
    "\n",
    "    seen_topk = topk.merge(impressions_df, on=['user', 'item'], how='inner')\n",
    "\n",
    "    if seen_topk.empty:\n",
    "        print('⚠️ None of the recommended pairs were observed under the logged policy.')\n",
    "        return 0.0, seen_topk, pd.DataFrame()\n",
    "\n",
    "    seen_topk['matched_like'] = seen_topk.apply(\n",
    "        lambda row: 1 if row['item'] in likes_by_user.get(row['user'], set()) else 0, axis=1\n",
    "    )\n",
    "\n",
    "    per_user = (\n",
    "        seen_topk.groupby('user')['matched_like']\n",
    "        .agg(['sum', 'count'])\n",
    "        .rename(columns={'sum': 'matched_likes', 'count': 'items_returned'})\n",
    "    )\n",
    "\n",
    "    num_users = len(per_user)\n",
    "    if num_users == 0:\n",
    "        print('⚠️ No users with seen recommendations for matched hits calculation.')\n",
    "        return 0.0, seen_topk, per_user\n",
    "\n",
    "    total_items_returned = per_user['items_returned'].sum()\n",
    "    matched_hits = per_user['matched_likes'].sum() / total_items_returned if total_items_returned else 0.0\n",
    "    per_user['matched_hits_at_k'] = per_user['matched_likes'] / per_user['items_returned']\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"### OFF-POLICY MATCHED HITS@K (SEEN IMPRESSIONS) ###\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Users evaluated:                  {num_users:,}\")\n",
    "    print(f\"Total seen recommendations:       {total_items_returned:,}\")\n",
    "    print(f\"Matched Hits@{k} (micro):          {matched_hits:.4f}\")\n",
    "    print(f\"Median per-user matched hits:     {per_user['matched_hits_at_k'].median():.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return matched_hits, seen_topk, per_user\n",
    "\n",
    "def logged_policy_matched_hits(interactions):\n",
    "    \"\"\"Compute matched hit statistics for the historical logged policy.\"\"\"\n",
    "    df = interactions[['decidermemberid', 'othermemberid', 'like']].to_pandas()\n",
    "\n",
    "    if df.empty:\n",
    "        print('⚠️ Interaction data is empty.')\n",
    "        return 0.0, pd.DataFrame()\n",
    "\n",
    "    per_user = (\n",
    "        df.groupby('decidermemberid')\n",
    "        .agg(impressions=('othermemberid', 'count'), matched_likes=('like', 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    per_user['matched_hits_rate'] = per_user['matched_likes'] / per_user['impressions']\n",
    "\n",
    "    total_impressions = per_user['impressions'].sum()\n",
    "    total_matched_likes = per_user['matched_likes'].sum()\n",
    "\n",
    "    micro_rate = total_matched_likes / total_impressions if total_impressions else 0.0\n",
    "    median_rate = per_user['matched_hits_rate'].median() if not per_user.empty else 0.0\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"### LOGGED POLICY MATCHED HITS ###\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Users evaluated:                  {len(per_user):,}\")\n",
    "    print(f\"Total logged impressions:         {total_impressions:,}\")\n",
    "    print(f\"Total logged likes (matches):     {int(total_matched_likes):,}\")\n",
    "    print(f\"Matched Hits (micro):             {micro_rate:.4f}\")\n",
    "    print(f\"Median per-user matched hits:     {median_rate:.4f}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return micro_rate, per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7bd2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CLASSIFICATION METRICS EVALUATION ON TEST SET\n",
      "\n",
      "Evaluating all 30,327 users in test set\n",
      "Generating recommendations for 30,327 users...\n",
      "Generating recommendations for 30,327 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building pairs: 100%|██████████| 30327/30327 [00:01<00:00, 25059.66it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\n",
      "======================================================================\n",
      "Total pairs evaluated:             2,442,961\n",
      "Positive pairs (likes):            25,337 (1.04%)\n",
      "Negative pairs (no like):          2,417,624\n",
      "\n",
      "Micro ROC AUC:                     0.7418\n",
      "Micro Average Precision (PR-AUC):  0.0252\n",
      "Brier score:                       0.2530\n",
      "Log loss:                          0.7781\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\n",
      "======================================================================\n",
      "Total pairs evaluated:             47,645\n",
      "Positive pairs (likes):            25,337 (53.18%)\n",
      "Negative pairs (no like):          22,308\n",
      "\n",
      "Micro ROC AUC:                     0.7051\n",
      "Micro Average Precision (PR-AUC):  0.6945\n",
      "Brier score:                       0.2329\n",
      "Log loss:                          0.6655\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### ML (ALS RECOMMENDER) PERFORMANCE ON TEST SET ###\n",
      "======================================================================\n",
      "Total pairs evaluated:             47,645\n",
      "Positive pairs (likes):            25,337 (53.18%)\n",
      "Negative pairs (no like):          22,308\n",
      "\n",
      "Micro ROC AUC:                     0.7051\n",
      "Micro Average Precision (PR-AUC):  0.6945\n",
      "Brier score:                       0.2329\n",
      "Log loss:                          0.6655\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### OFF-POLICY MATCHED HITS@K (SEEN IMPRESSIONS) ###\n",
      "======================================================================\n",
      "Users evaluated:                  13,154\n",
      "Total seen recommendations:       47,645\n",
      "Matched Hits@100 (micro):          0.5318\n",
      "Median per-user matched hits:     0.5000\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "### OFF-POLICY MATCHED HITS@K (SEEN IMPRESSIONS) ###\n",
      "======================================================================\n",
      "Users evaluated:                  13,154\n",
      "Total seen recommendations:       47,645\n",
      "Matched Hits@100 (micro):          0.5318\n",
      "Median per-user matched hits:     0.5000\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate the raw recommendation pairs\n",
    "pairs_df, overall_metrics = evaluate_classification_metrics(engine_test, test_data, sample_size=40000, k=100)\n",
    "\n",
    "print()\n",
    "\n",
    "# re-score using only impressions the user actually saw\n",
    "seen_pairs_df, seen_metrics = evaluate_classification_metrics_seen_only(pairs_df)\n",
    "\n",
    "print()\n",
    "\n",
    "# compute matched hits@K (off-policy precision style metric)\n",
    "matched_hits, topk_matched_pairs, per_user_matched = matched_hits_at_k(pairs_df, k=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchmaker-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
