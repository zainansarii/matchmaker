{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9102631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zainansari/matchmaker\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99fd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "import cugraph\n",
    "import cupy as cp\n",
    "import itertools\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5fc66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pandas dataframe into a GPU dataframe\n",
    "gdf = cudf.read_csv(\"examples/data/swipes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d575e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep relevant columns for graph\n",
    "edges = gdf[['decidermemberid', 'othermemberid', 'like']]\n",
    "\n",
    "G = cugraph.Graph(directed=True)\n",
    "\n",
    "# Pick 'like' as edge weight\n",
    "G.from_cudf_edgelist(\n",
    "    gdf,\n",
    "    source=\"decidermemberid\",\n",
    "    destination=\"othermemberid\",\n",
    "    edge_attr=\"like\",  # only one column allowed\n",
    "    store_transposed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a483fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decidermemberid</th>\n",
       "      <th>othermemberid</th>\n",
       "      <th>like</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3847776</td>\n",
       "      <td>3227524</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608590</td>\n",
       "      <td>519321</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>397116</td>\n",
       "      <td>453914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3847776</td>\n",
       "      <td>1269455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1630969</td>\n",
       "      <td>347909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488281</th>\n",
       "      <td>3797393</td>\n",
       "      <td>922119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488282</th>\n",
       "      <td>540827</td>\n",
       "      <td>1560976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488283</th>\n",
       "      <td>1679523</td>\n",
       "      <td>1173164</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488284</th>\n",
       "      <td>3675928</td>\n",
       "      <td>1582474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488285</th>\n",
       "      <td>734875</td>\n",
       "      <td>1341801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5488286 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         decidermemberid  othermemberid  like\n",
       "0                3847776        3227524     1\n",
       "1                 608590         519321     0\n",
       "2                 397116         453914     0\n",
       "3                3847776        1269455     1\n",
       "4                1630969         347909     0\n",
       "...                  ...            ...   ...\n",
       "5488281          3797393         922119     0\n",
       "5488282           540827        1560976     1\n",
       "5488283          1679523        1173164     1\n",
       "5488284          3675928        1582474     0\n",
       "5488285           734875        1341801     1\n",
       "\n",
       "[5488286 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819114b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PageRank on GPU\n",
    "pr = cugraph.pagerank(G, alpha=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5a809d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vertex</th>\n",
       "      <th>pagerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3865583</td>\n",
       "      <td>0.000263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3844050</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3861867</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3866079</td>\n",
       "      <td>0.000341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1531328</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167426</th>\n",
       "      <td>3884646</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167427</th>\n",
       "      <td>3884647</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167428</th>\n",
       "      <td>3884650</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167429</th>\n",
       "      <td>3884674</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167430</th>\n",
       "      <td>3884677</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167431 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         vertex  pagerank\n",
       "0       3865583  0.000263\n",
       "1       3844050  0.000366\n",
       "2       3861867  0.000156\n",
       "3       3866079  0.000341\n",
       "4       1531328  0.000132\n",
       "...         ...       ...\n",
       "167426  3884646  0.000004\n",
       "167427  3884647  0.000004\n",
       "167428  3884650  0.000004\n",
       "167429  3884674  0.000004\n",
       "167430  3884677  0.000004\n",
       "\n",
       "[167431 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78095032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likes received per user\n",
    "likes_received = edges.groupby('othermemberid')['like'].sum().reset_index()\n",
    "likes_received = likes_received.rename(columns={'othermemberid':'vertex', 'like':'likes_received'})\n",
    "\n",
    "# Likes given per user\n",
    "likes_given = edges.groupby('decidermemberid')['like'].sum().reset_index()\n",
    "likes_given = likes_given.rename(columns={'decidermemberid':'vertex', 'like':'likes_given'})\n",
    "\n",
    "# Merge stats with PageRank\n",
    "stats = pr.merge(likes_received, on='vertex', how='left') \\\n",
    "          .merge(likes_given, on='vertex', how='left')\n",
    "\n",
    "# Fill NaNs with 0\n",
    "stats = stats.fillna(0)\n",
    "\n",
    "# Compute like ratio (add small epsilon to avoid div by 0)\n",
    "stats['like_ratio'] = stats['likes_received'] / (stats['likes_given'] + 1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324232b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map user -> gender\n",
    "decider_gender = gdf[['decidermemberid', 'decidergender']].rename(\n",
    "    columns={'decidermemberid': 'vertex', 'decidergender': 'gender'}\n",
    ")\n",
    "other_gender = gdf[['othermemberid', 'othergender']].rename(\n",
    "    columns={'othermemberid': 'vertex', 'othergender': 'gender'}\n",
    ")\n",
    "\n",
    "# Combine (take first non-null)\n",
    "gender_map = cudf.concat([decider_gender, other_gender]).drop_duplicates(subset='vertex', keep='first')\n",
    "\n",
    "# Merge gender with stats\n",
    "stats = stats.merge(gender_map, on='vertex', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a92d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_users = stats.sort_values('pagerank', ascending=False).head(10)\n",
    "print(top_users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44664a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from scipy import sparse as sp\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.gpu import Matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class DatingAppCollaborativeFilter:\n",
    "    \"\"\"\n",
    "    GPU-accelerated collaborative filtering for dating app recommendations.\n",
    "    Uses Implicit ALS with square user-user matrix structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 factors=64,           \n",
    "                 regularization=0.01,  \n",
    "                 iterations=15,        \n",
    "                 alpha=40.0,          \n",
    "                 use_gpu=True,\n",
    "                 random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the collaborative filtering model.\n",
    "        \n",
    "        Args:\n",
    "            factors: Number of latent factors\n",
    "            regularization: L2 regularization strength\n",
    "            iterations: Number of ALS iterations\n",
    "            alpha: Confidence weighting for implicit feedback\n",
    "            use_gpu: Whether to use GPU acceleration\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.factors = factors\n",
    "        self.regularization = regularization\n",
    "        self.iterations = iterations\n",
    "        self.alpha = alpha\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Check GPU availability\n",
    "        self.use_gpu = use_gpu and self._check_gpu_available()\n",
    "        \n",
    "        self.model = AlternatingLeastSquares(\n",
    "            factors=factors,\n",
    "            regularization=regularization,\n",
    "            iterations=iterations,\n",
    "            use_gpu=self.use_gpu,\n",
    "            random_state=random_state,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Mappings and matrices\n",
    "        self.user2idx = None\n",
    "        self.idx2user = None\n",
    "        self.user_user_matrix = None\n",
    "        \n",
    "        # Cache for performance\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        \n",
    "        # Stats\n",
    "        self.n_users = 0\n",
    "        self.n_interactions = 0\n",
    "        \n",
    "    def _check_gpu_available(self):\n",
    "        \"\"\"Check if GPU is available for computation\"\"\"\n",
    "        try:\n",
    "            import cupy\n",
    "            test = cupy.array([1, 2, 3])\n",
    "            del test\n",
    "            print(\"âœ… GPU acceleration enabled\")\n",
    "            return True\n",
    "        except:\n",
    "            print(\"âš ï¸ GPU not available, falling back to CPU\")\n",
    "            return False\n",
    "    \n",
    "    def _convert_factors_to_numpy(self, factors):\n",
    "        \"\"\"Convert implicit factors to numpy arrays, handling both CPU and GPU matrices\"\"\"\n",
    "        try:\n",
    "            # Method 1: Use .to_numpy() if available (for implicit GPU matrices)\n",
    "            if hasattr(factors, 'to_numpy'):\n",
    "                return factors.to_numpy()\n",
    "            # Method 2: Use cupy.asnumpy() for CuPy arrays\n",
    "            elif hasattr(factors, 'get'):\n",
    "                return factors.get()\n",
    "            # Method 3: Direct numpy conversion\n",
    "            else:\n",
    "                return np.array(factors)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Factor conversion failed with {type(factors)}: {e}\")\n",
    "            # Fallback - return the original factors (may cause issues)\n",
    "            return factors\n",
    "    \n",
    "    def fit(self, interactions, \n",
    "            user_col='decidermemberid', \n",
    "            target_col='othermemberid', \n",
    "            value_col='like',\n",
    "            min_interactions=2):\n",
    "        \"\"\"\n",
    "        Fit the collaborative filtering model on interaction data.\n",
    "        \n",
    "        Args:\n",
    "            interactions: DataFrame with user interactions (can be pandas or cudf)\n",
    "            user_col: Column name for users making decisions\n",
    "            target_col: Column name for target users\n",
    "            value_col: Column name for interaction values (0/1)\n",
    "            min_interactions: Minimum interactions required per user\n",
    "        \"\"\"\n",
    "        print(\"ðŸš€ Starting collaborative filtering training...\")\n",
    "        total_start = pd.Timestamp.now()\n",
    "        \n",
    "        # Handle both pandas and cudf DataFrames\n",
    "        if hasattr(interactions, 'to_pandas'):\n",
    "            # It's a cuDF DataFrame\n",
    "            print(\"ðŸ“Š Detected cuDF DataFrame, converting to pandas...\")\n",
    "            df = interactions[[user_col, target_col, value_col]].to_pandas()\n",
    "        else:\n",
    "            # It's a pandas DataFrame\n",
    "            df = interactions[[user_col, target_col, value_col]].copy()\n",
    "        \n",
    "        # Filter out users with too few interactions\n",
    "        user_counts = df.groupby(user_col)[value_col].count().reset_index()\n",
    "        user_counts.columns = [user_col, 'count']\n",
    "        active_users = user_counts[user_counts['count'] >= min_interactions][user_col]\n",
    "        \n",
    "        # Filter interactions\n",
    "        df = df[df[user_col].isin(active_users)]\n",
    "        \n",
    "        # Create user mappings - get all unique users from both columns\n",
    "        print(\"ðŸ“Š Creating user mappings...\")\n",
    "        all_users = set(df[user_col].unique()) | set(df[target_col].unique())\n",
    "        # Filter to only include active users\n",
    "        all_users = list(all_users.intersection(set(active_users)))\n",
    "        \n",
    "        self.n_users = len(all_users)\n",
    "        \n",
    "        # Create mappings\n",
    "        self.user2idx = {user: idx for idx, user in enumerate(all_users)}\n",
    "        self.idx2user = {idx: user for user, idx in self.user2idx.items()}\n",
    "        \n",
    "        print(f\"ðŸ“ˆ {self.n_users:,} users in square matrix\")\n",
    "        \n",
    "        # Map to indices\n",
    "        df['user_idx'] = df[user_col].map(self.user2idx)\n",
    "        df['target_idx'] = df[target_col].map(self.user2idx)\n",
    "        \n",
    "        # Remove unmapped entries (users not in active list)\n",
    "        df = df.dropna(subset=['user_idx', 'target_idx'])\n",
    "        df['user_idx'] = df['user_idx'].astype(np.int32)\n",
    "        df['target_idx'] = df['target_idx'].astype(np.int32)\n",
    "        \n",
    "        # Apply confidence weighting (implicit feedback)\n",
    "        df['confidence'] = 1 + self.alpha * df[value_col].astype(np.float32)\n",
    "        \n",
    "        # Remove duplicates by taking max confidence\n",
    "        df = df.groupby(['user_idx', 'target_idx'])['confidence'].max().reset_index()\n",
    "        \n",
    "        self.n_interactions = len(df)\n",
    "        print(f\"ðŸ’¾ {self.n_interactions:,} unique interactions\")\n",
    "        \n",
    "        # Build square sparse matrix (user x user)\n",
    "        print(\"ðŸ”¨ Building square sparse matrix...\")\n",
    "        \n",
    "        self.user_user_matrix = sp.csr_matrix(\n",
    "            (df['confidence'].values, \n",
    "             (df['user_idx'].values, df['target_idx'].values)),\n",
    "            shape=(self.n_users, self.n_users),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        density = 100.0 * self.n_interactions / (self.n_users * self.n_users)\n",
    "        print(f\"ðŸ“Š Matrix density: {density:.4f}%\")\n",
    "        \n",
    "        # Fit the model - for ALS, we need to transpose for item-user format\n",
    "        print(\"ðŸŽ¯ Training ALS model...\")\n",
    "        fit_start = pd.Timestamp.now()\n",
    "        \n",
    "        self.model.fit(self.user_user_matrix.T.tocsr(), show_progress=True)\n",
    "        \n",
    "        fit_time = (pd.Timestamp.now() - fit_start).total_seconds()\n",
    "        total_time = (pd.Timestamp.now() - total_start).total_seconds()\n",
    "        \n",
    "        # Cache factors for faster recommendations - convert to numpy properly\n",
    "        print(\"ðŸ”„ Converting factors to numpy arrays...\")\n",
    "        self.user_factors = self._convert_factors_to_numpy(self.model.user_factors)\n",
    "        self.item_factors = self._convert_factors_to_numpy(self.model.item_factors)\n",
    "        \n",
    "        print(f\"âœ… Model trained in {fit_time:.2f}s\")\n",
    "        print(f\"â±ï¸ Total processing time: {total_time:.2f}s\")\n",
    "        print(f\"ðŸ“‹ Factor arrays: user_factors {self.user_factors.shape}, item_factors {self.item_factors.shape}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def recommend_for_user(self, user_id, N=10, filter_already_liked=True, return_scores=True):\n",
    "        \"\"\"\n",
    "        Get top N recommendations for a specific user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: User ID to get recommendations for\n",
    "            N: Number of recommendations\n",
    "            filter_already_liked: Whether to filter out already liked users\n",
    "            return_scores: Whether to return scores with recommendations\n",
    "        \n",
    "        Returns:\n",
    "            List of (user_id, score) tuples or just user_ids\n",
    "        \"\"\"\n",
    "        if user_id not in self.user2idx:\n",
    "            print(f\"âš ï¸ User {user_id} not found in training data\")\n",
    "            return []\n",
    "        \n",
    "        user_idx = self.user2idx[user_id]\n",
    "        \n",
    "        # Extract the user's row from the matrix\n",
    "        user_items = self.user_user_matrix[user_idx]\n",
    "        \n",
    "        # Get recommendations using correct method signature\n",
    "        try:\n",
    "            recommendations, scores = self.model.recommend(\n",
    "                user_idx,\n",
    "                user_items,\n",
    "                N=N,\n",
    "                recalculate_user=False\n",
    "            )\n",
    "        except TypeError:\n",
    "            # Fallback for different implicit versions\n",
    "            recommendations, scores = self.model.recommend(\n",
    "                user_idx,\n",
    "                user_items,\n",
    "                N=N\n",
    "            )\n",
    "        \n",
    "        # Manual filtering of already liked items if requested\n",
    "        if filter_already_liked:\n",
    "            already_liked = set(user_items.indices[user_items.data > 1])\n",
    "            \n",
    "            filtered_results = []\n",
    "            for target_idx, score in zip(recommendations, scores):\n",
    "                if int(target_idx) not in already_liked:\n",
    "                    filtered_results.append((target_idx, score))\n",
    "            \n",
    "            # Take only the top N after filtering\n",
    "            filtered_results = filtered_results[:N]\n",
    "            recommendations = [r[0] for r in filtered_results]\n",
    "            scores = [r[1] for r in filtered_results]\n",
    "        \n",
    "        # Map back to user IDs\n",
    "        results = []\n",
    "        for target_idx, score in zip(recommendations, scores):\n",
    "            target_id = self.idx2user.get(int(target_idx))\n",
    "            if target_id is not None:\n",
    "                if return_scores:\n",
    "                    results.append((target_id, float(score)))\n",
    "                else:\n",
    "                    results.append(target_id)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def recommend_batch(self, user_ids, N=10, filter_already_liked=True, batch_size=1000):\n",
    "        \"\"\"\n",
    "        Ultra-optimized batch recommendations using matrix operations with memory management.\n",
    "        \n",
    "        This version processes users in batches to avoid memory issues while still\n",
    "        being much faster than individual processing.\n",
    "        \n",
    "        Args:\n",
    "            user_ids: List of user IDs\n",
    "            N: Number of recommendations per user\n",
    "            filter_already_liked: Whether to filter out already liked users\n",
    "            batch_size: Number of users to process at once (controls memory usage)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping user_id to list of recommendations\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Filter valid users\n",
    "        valid_users = [uid for uid in user_ids if uid in self.user2idx]\n",
    "        if not valid_users:\n",
    "            return results\n",
    "        \n",
    "        print(f\"ðŸš€ Processing {len(valid_users)} users in batches of {batch_size}\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for batch_start in tqdm(range(0, len(valid_users), batch_size)):\n",
    "            batch_end = min(batch_start + batch_size, len(valid_users))\n",
    "            batch_users = valid_users[batch_start:batch_end]\n",
    "            \n",
    "            try:\n",
    "                # Convert to indices for this batch\n",
    "                user_indices = np.array([self.user2idx[uid] for uid in batch_users])\n",
    "                \n",
    "                # Get user factors for this batch\n",
    "                user_factors_batch = self.user_factors[user_indices]  # Shape: (batch_size, n_factors)\n",
    "                \n",
    "                # Compute scores for all users in batch against all items\n",
    "                batch_scores = np.dot(user_factors_batch, self.item_factors.T)  # Shape: (batch_size, n_users)\n",
    "                \n",
    "                # Process each user's scores in this batch\n",
    "                for i, uid in enumerate(batch_users):\n",
    "                    user_idx = user_indices[i]\n",
    "                    user_scores = batch_scores[i].copy()  # Copy to avoid modifying original\n",
    "                    \n",
    "                    # Apply filtering if requested\n",
    "                    if filter_already_liked:\n",
    "                        user_items = self.user_user_matrix[user_idx]\n",
    "                        already_liked = user_items.indices[user_items.data > 1]\n",
    "                        user_scores[already_liked] = -np.inf  # Mask out already liked items\n",
    "                    \n",
    "                    # Also mask out the user themselves\n",
    "                    user_scores[user_idx] = -np.inf\n",
    "                    \n",
    "                    # Get top N recommendations efficiently\n",
    "                    if len(user_scores) > N:\n",
    "                        # Use argpartition for better performance when N << total_users\n",
    "                        top_indices = np.argpartition(user_scores, -N)[-N:]\n",
    "                        top_indices = top_indices[np.argsort(user_scores[top_indices])][::-1]\n",
    "                    else:\n",
    "                        # If we have fewer items than N, just sort all\n",
    "                        top_indices = np.argsort(user_scores)[::-1]\n",
    "                    \n",
    "                    # Convert to user IDs and scores\n",
    "                    user_results = []\n",
    "                    for idx in top_indices:\n",
    "                        if user_scores[idx] > -np.inf:  # Valid recommendation\n",
    "                            target_id = self.idx2user.get(int(idx))\n",
    "                            if target_id is not None:\n",
    "                                user_results.append((target_id, float(user_scores[idx])))\n",
    "                        if len(user_results) >= N:  # Stop once we have enough\n",
    "                            break\n",
    "                    \n",
    "                    results[uid] = user_results\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Batch processing failed for batch {batch_start//batch_size + 1} ({e})\")\n",
    "                print(\"ðŸ”„ Falling back to individual processing for this batch...\")\n",
    "                \n",
    "                # Fallback to individual processing for this batch only\n",
    "                for uid in batch_users:\n",
    "                    try:\n",
    "                        results[uid] = self.recommend_for_user(uid, N, filter_already_liked)\n",
    "                    except Exception as individual_error:\n",
    "                        print(f\"âš ï¸ Individual processing failed for user {uid}: {individual_error}\")\n",
    "                        results[uid] = []\n",
    "        \n",
    "        print(f\"âœ… Completed batch processing for {len(results)} users\")\n",
    "        return results\n",
    "    \n",
    "    def find_similar_users(self, user_id, N=10):\n",
    "        \"\"\"\n",
    "        Find users with similar preferences.\n",
    "        \n",
    "        Args:\n",
    "            user_id: User ID to find similar users for\n",
    "            N: Number of similar users to return\n",
    "        \n",
    "        Returns:\n",
    "            List of (user_id, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        if user_id not in self.user2idx:\n",
    "            return []\n",
    "        \n",
    "        user_idx = self.user2idx[user_id]\n",
    "        \n",
    "        # Use numpy arrays for consistent computation\n",
    "        user_vector = self.user_factors[user_idx]\n",
    "        all_factors = self.user_factors\n",
    "        \n",
    "        # Compute similarities with all other users - CPU version for consistency\n",
    "        try:\n",
    "            norms = np.linalg.norm(all_factors, axis=1)\n",
    "            user_norm = np.linalg.norm(user_vector)\n",
    "            similarities = np.dot(all_factors, user_vector) / (norms * user_norm + 1e-8)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing similarities: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Get top N similar users (excluding self)\n",
    "        similarities[user_idx] = -1  # Exclude self\n",
    "        top_indices = np.argsort(similarities)[-N-1:-1][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if idx != user_idx and similarities[idx] > 0:\n",
    "                similar_user_id = self.idx2user.get(int(idx))\n",
    "                if similar_user_id:\n",
    "                    results.append((similar_user_id, float(similarities[idx])))\n",
    "        \n",
    "        return results[:N]\n",
    "    \n",
    "    def get_user_stats(self, user_id):\n",
    "        \"\"\"\n",
    "        Get statistics for a specific user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: User ID to get stats for\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with user statistics\n",
    "        \"\"\"\n",
    "        if user_id not in self.user2idx:\n",
    "            return None\n",
    "        \n",
    "        user_idx = self.user2idx[user_id]\n",
    "        \n",
    "        # Count likes given (row in matrix)\n",
    "        user_row = self.user_user_matrix[user_idx]\n",
    "        likes_given = (user_row.data > 1).sum()\n",
    "        total_evaluated = user_row.nnz\n",
    "        \n",
    "        # Count likes received (column in matrix)\n",
    "        user_col = self.user_user_matrix[:, user_idx]\n",
    "        likes_received = (user_col.data > 1).sum()\n",
    "        \n",
    "        return {\n",
    "            'user_id': user_id,\n",
    "            'profiles_evaluated': int(total_evaluated),\n",
    "            'likes_given': int(likes_given),\n",
    "            'likes_received': int(likes_received),\n",
    "            'like_rate': float(likes_given / max(total_evaluated, 1)),\n",
    "            'selectivity': 1.0 - float(likes_given / max(total_evaluated, 1))\n",
    "        }\n",
    "    \n",
    "    def explain_recommendation(self, user_id, recommended_user_id, N=5):\n",
    "        \"\"\"\n",
    "        Explain why a user was recommended to another user.\n",
    "        \n",
    "        Args:\n",
    "            user_id: User ID\n",
    "            recommended_user_id: Recommended user ID\n",
    "            N: Number of similar users to show as explanation\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with explanation details\n",
    "        \"\"\"\n",
    "        if user_id not in self.user2idx or recommended_user_id not in self.user2idx:\n",
    "            return None\n",
    "        \n",
    "        user_idx = self.user2idx[user_id]\n",
    "        rec_user_idx = self.user2idx[recommended_user_id]\n",
    "        \n",
    "        # Get users who liked the recommended user\n",
    "        rec_user_col = self.user_user_matrix[:, rec_user_idx].tocoo()\n",
    "        users_who_liked = [(self.idx2user[idx], float(data)) \n",
    "                          for idx, data in zip(rec_user_col.row, rec_user_col.data) \n",
    "                          if data > 1 and idx in self.idx2user]\n",
    "        \n",
    "        # Find similar users among those who liked\n",
    "        similar_users = self.find_similar_users(user_id, N=50)\n",
    "        similar_who_liked = []\n",
    "        \n",
    "        for similar_user, similarity in similar_users:\n",
    "            if any(u[0] == similar_user for u in users_who_liked):\n",
    "                similar_who_liked.append((similar_user, similarity))\n",
    "                if len(similar_who_liked) >= N:\n",
    "                    break\n",
    "        \n",
    "        # Calculate predicted score using numpy arrays\n",
    "        user_vector = self.user_factors[user_idx]\n",
    "        rec_user_vector = self.item_factors[rec_user_idx]\n",
    "        predicted_score = float(np.dot(user_vector, rec_user_vector))\n",
    "        \n",
    "        return {\n",
    "            'user_id': user_id,\n",
    "            'recommended_user_id': recommended_user_id,\n",
    "            'predicted_score': predicted_score,\n",
    "            'total_likes': len(users_who_liked),\n",
    "            'similar_users_who_liked': similar_who_liked,\n",
    "            'explanation': f\"Recommended because {len(similar_who_liked)} similar users also liked this profile\"\n",
    "        }\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the trained model to disk\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        model_data = {\n",
    "            'model_state': self.model,\n",
    "            'user2idx': self.user2idx,\n",
    "            'idx2user': self.idx2user,\n",
    "            'n_users': self.n_users,\n",
    "            'n_interactions': self.n_interactions,\n",
    "            'user_factors': self.user_factors,\n",
    "            'item_factors': self.item_factors\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        print(f\"âœ… Model saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load a trained model from disk\"\"\"\n",
    "        import pickle\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        self.model = model_data['model_state']\n",
    "        self.user2idx = model_data['user2idx']\n",
    "        self.idx2user = model_data['idx2user']\n",
    "        self.n_users = model_data['n_users']\n",
    "        self.n_interactions = model_data['n_interactions']\n",
    "        self.user_factors = model_data['user_factors']\n",
    "        self.item_factors = model_data['item_factors']\n",
    "        \n",
    "        print(f\"âœ… Model loaded from {filepath}\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567beb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the collaborative filtering model\n",
    "print(\"ðŸš€ Training Collaborative Filtering Model...\")\n",
    "\n",
    "cf_model = DatingAppCollaborativeFilter(\n",
    "    factors=32,\n",
    "    regularization=0.01,\n",
    "    iterations=10,\n",
    "    use_gpu=True\n",
    ")\n",
    "\n",
    "cf_model.fit(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Get recommendations for a specific user\n",
    "user_id = 3847776\n",
    "recommendations = cf_model.recommend_for_user(user_id, N=5)\n",
    "\n",
    "print(f\"Top 5 recommendations for user {user_id}:\")\n",
    "for i, (recommended_user, score) in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. User {recommended_user}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dee86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Find similar users\n",
    "user_id = 228705\n",
    "similar_users = cf_model.find_similar_users(user_id, N=3)\n",
    "\n",
    "print(f\"Users similar to {user_id}:\")\n",
    "for i, (similar_user, similarity) in enumerate(similar_users, 1):\n",
    "    print(f\"{i}. User {similar_user}: {similarity:.4f} similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab166f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Get user statistics\n",
    "user_stats = cf_model.get_user_stats(228705)\n",
    "\n",
    "print(f\"User statistics:\")\n",
    "for key, value in user_stats.items():\n",
    "    print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Explain a recommendation\n",
    "explanation = cf_model.explain_recommendation(3847776, 1899603, N=3)\n",
    "\n",
    "if explanation:\n",
    "    print(f\"Why user 1899603 was recommended to user 3847776:\")\n",
    "    print(f\"- Predicted score: {explanation['predicted_score']:.4f}\")\n",
    "    print(f\"- Total users who liked this profile: {explanation['total_likes']}\")\n",
    "    print(f\"- Similar users who also liked: {len(explanation['similar_users_who_liked'])}\")\n",
    "    print(f\"- {explanation['explanation']}\")\n",
    "else:\n",
    "    print(\"No explanation available for this recommendation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Batch recommendations for multiple users\n",
    "user_ids = gdf.decidermemberid.unique().to_arrow().to_pylist()\n",
    "batch_recommendations = cf_model.recommend_batch(user_ids, N=10, batch_size=100)\n",
    "\n",
    "print(\"Batch recommendations:\")\n",
    "for user_id, recs in batch_recommendations.items():\n",
    "    print(f\"\\nUser {user_id}:\")\n",
    "    for i, (rec_user, score) in enumerate(recs, 1):\n",
    "        print(f\"  {i}. User {rec_user}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matchmaker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
